{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece VS Huggingface tokenizer\n",
    "\n",
    "한국어 서브워드 분절 알고리즘 실습&비교\n",
    "\n",
    "200825\n",
    "\n",
    "고우영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 학습시간 비교(s) | 8000   | 16000 | 32000 | 64000 | 128000|\n",
    "|------|------|------|------|------|------|\n",
    "|   Sentencepiece  | 11| 22 |48| 110 |282|\n",
    "|   hugging_face  | 10| 11 |11| 12 |12|\n",
    "\n",
    "| 추론시간 비교(s) | 8000| 128000|\n",
    "|------|------|------|\n",
    "|   Sentencepiece  | 4.5| 4.93 |\n",
    "|   hugging_face  | 4.9| 4.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSMC 데이터셋 로드\n",
    "## 15만 문장, 113만 word(띄어쓰기 기준), 평균 7.5word/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading done!\n",
      "문장: ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다']\n",
      "라벨: [0, 1, 0]\n",
      "['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다']\n",
      "\n",
      "코퍼스 문장수/평균/총 단어 갯수 : 149996, 7.6 / 1137736\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NSMC 데이터 로드\n",
    "import pandas as pd\n",
    "f_train = pd.read_csv('data/nsmc.txt', sep='\\t')\n",
    "train_pair = [(row[1], row[2]) for _, row in f_train.iterrows() if type(row[1]) == str]  # nan 제거\n",
    "\n",
    "#  문장 및 라벨 데이터 추출\n",
    "train_data  = [pair[0] for pair in train_pair]\n",
    "train_label = [pair[1] for pair in train_pair]\n",
    "print('data loading done!')\n",
    "print('문장: %s' %(train_data[:3]))\n",
    "print('라벨: %s' %(train_label[:3]))\n",
    "\n",
    "# subword 학습을 위해 문장만 따로 저장\n",
    "with open('data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_data:\n",
    "        f.write(line+'\\n')\n",
    "\n",
    "# subword 학습을 위해 문장만 따로 저장\n",
    "with open('data/train_tokenizer.txt', 'r', encoding='utf-8') as f:\n",
    "    test_tokenizer = f.read().split('\\n')\n",
    "print(test_tokenizer[:3])\n",
    "\n",
    "num_word_list = [len(sentence.split()) for sentence in test_tokenizer]\n",
    "print('\\n코퍼스 문장수/평균/총 단어 갯수 : %d, %.1f / %d' % (len(num_word_list), sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece 학습"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 설치\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sentencepiece as spm\n",
    "\n",
    "input_file = 'data/train_tokenizer.txt'\n",
    "vocab_size = 32000\n",
    "model_name = 'model_sentencepiece/sentencepiece_tokenizer_kor_%d' % (vocab_size)\n",
    "model_type = 'bpe'\n",
    "character_coverage  = 1.0  # 0.9995\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, model_name, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁나는', '▁오늘', '▁아침', '밥', '을', '▁먹', '었다', '.']\n"
     ]
    }
   ],
   "source": [
    "## check\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(model_name))\n",
    "sentencepiece_tokenizer = sp.encode\n",
    "token = sentencepiece_tokenizer('나는 오늘 아침밥을 먹었다.', out_type=str)\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Huggingface setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hugging face tokenizer 설치\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huggingface train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "corpus_file   = 'data/train_tokenizer.txt'\n",
    "vocab_size    = 32000\n",
    "limit_alphabet= 6000\n",
    "output_path   = 'model_hugging_face/hugging_%d'%(vocab_size)\n",
    "min_frequency = 5\n",
    "\n",
    "## WordPiece 모델 선언\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,  # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\")\n",
    "\n",
    "## 모델 학습\n",
    "tokenizer.train(\n",
    "    files=[corpus_file],\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency = min_frequency,  # 단어의 최소 발생 빈도, 3\n",
    "    show_progress = True,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]',     \n",
    "                    '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',\n",
    "                    '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]',\n",
    "                    '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]',\n",
    "                    '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]',\n",
    "                    '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]',\n",
    "                    '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]',\n",
    "                    '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]',\n",
    "                    '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]',\n",
    "                    '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]',\n",
    "                    '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]'\n",
    "                   ],  # 스페셜 토큰\n",
    ")\n",
    "## tokenizer, vocab 저장\n",
    "tokenizer.save_model('.', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huggingface Tokenize test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx   : [2, 5951, 6105, 7623, 3482, 3216, 1472, 5595, 130, 3]\n",
      "tokens: ['[CLS]', '나는', '오늘', '아침', '##밥', '##을', '먹', '##었다', '.', '[SEP]']\n",
      "offset: [(0, 0), (0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 14), (14, 15), (0, 0)]\n",
      "idx   : [2, 25910, 5691, 12664, 130, 130, 5821, 6481, 5659, 130, 130, 5577, 13610, 3]\n",
      "tokens: ['[CLS]', '교도소', '이야기', '##구먼', '.', '.', '솔직히', '재미는', '없다', '.', '.', '평점', '조정', '[SEP]']\n",
      "offset: [(0, 0), (0, 3), (4, 7), (7, 9), (10, 11), (11, 12), (12, 15), (16, 19), (20, 22), (22, 23), (23, 24), (24, 26), (27, 29), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=output_path+'-vocab.txt',\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Cased 모델 시 False\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)\n",
    "\n",
    "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenze usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SentencePiece Usage, load & 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "['▁흠', '...', '포스터', '보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는', '것을', '추천', '한다']\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format('model_sentencepiece/sentencepiece_tokenizer_kor_32000'))\n",
    "sentencepiece_tokenizer = sp.encode\n",
    "\n",
    "result_tokenized_sentencepiece= sentencepiece_tokenizer(test_tokenizer, out_type=str)\n",
    "for tmp in result_tokenized_sentencepiece[:3]:\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huggingface Usage, load & 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "['▁흠', '...', '포스터', '보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는', '것을', '추천', '한다']\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format('model_sentencepiece/sentencepiece_tokenizer_kor_32000'))\n",
    "sentencepiece_tokenizer = sp.encode\n",
    "\n",
    "result_tokenized_sentencepiece= sentencepiece_tokenizer(test_tokenizer, out_type=str)\n",
    "for tmp in result_tokenized_sentencepiece[:3]:\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
